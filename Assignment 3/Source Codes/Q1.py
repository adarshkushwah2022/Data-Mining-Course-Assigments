# -*- coding: utf-8 -*-
"""DMG ASSIGNMENT 3 Q1 FINAL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Mp8Lzo9sTq0f4nbrOIBFiQEuIeEaPAaD

**Q1(40 points) Using any library, Perform any three clustering modelling and one Gaussian based clustering modelling and report the following for each model.                                                                         1.Centroid/representative object/prototype of each cluster for every model.                                                                         2.Visualization of the clusters.(You can use lesser data points/ dimensions for visualizations).                                                               3.Compare your cluster distribution with the true label count.                 4.Compare the cluster formation of the gaussian based method with the other three clustering methods and report your observations on the results.**

**Importing Pre-Requisite Libraries, Checking dataset size, Dropping duplicate rows**
"""

#Downloading Pre-requisite library
!pip install fuzzy-c-means

# Commented out IPython magic to ensure Python compatibility.
#Import Pre-requisite libraries
import warnings
import numpy as np
# %matplotlib inline
import pandas as pd
from fcmeans import FCM
from numpy import asarray
import matplotlib.pyplot as plt
warnings.filterwarnings("ignore")
from sklearn.cluster import KMeans
from sklearn.metrics import f1_score
from matplotlib import pyplot as plt
from sklearn.metrics import accuracy_score
from sklearn.mixture import GaussianMixture
from sklearn.preprocessing import OrdinalEncoder
from sklearn.cluster import AgglomerativeClustering
from sklearn.model_selection import train_test_split
from scipy.cluster.vq import whiten, kmeans, vq, kmeans2
from sklearn.feature_selection import mutual_info_classif

#ignoring warning messages if occur
warnings.filterwarnings("ignore")

#importing dataset
df=pd.read_csv('/content/drive/MyDrive/DMG ASSIGNMENT 3/covtype_train.csv')

#displaying number of rows and columns in dataset
df.shape

#Checking Duplicates
df.duplicated().sum()

#dropping duplicates
df.drop_duplicates(inplace=True)

#displaying number of rows and columns in dataset
df.shape

#displaying top 5 rows of dataframe
df.head()

"""**Ordinal Encoding to convert string categorical features to ordinal features**"""

#ordinal encoding of string categorical features
l=asarray(df['Elevation'])
l1=np.reshape(l,(len(l),1))
enc = OrdinalEncoder()
enc.fit(l1)
result = enc.fit_transform(l1)
for x in range(len(result)):
  result[x]=int(result[x])
df['Elevation']=result
df['Elevation'] = df['Elevation'].astype(int)

l=asarray(df['Aspect'])
l1=np.reshape(l,(len(l),1))
enc = OrdinalEncoder()
enc.fit(l1)
result = enc.fit_transform(l1)
for x in range(len(result)):
  result[x]=int(result[x])
df['Aspect']=result
df['Aspect'] = df['Aspect'].astype(int)

l=asarray(df['Slope'])
l1=np.reshape(l,(len(l),1))
enc = OrdinalEncoder()
enc.fit(l1)
result = enc.fit_transform(l1)
for x in range(len(result)):
  result[x]=int(result[x])
df['Slope']=result
df['Slope'] = df['Slope'].astype(int)

l=asarray(df['Hillshade_9am'])
l1=np.reshape(l,(len(l),1))
enc = OrdinalEncoder()
enc.fit(l1)
result = enc.fit_transform(l1)
for x in range(len(result)):
  result[x]=int(result[x])
df['Hillshade_9am']=result
df['Hillshade_9am'] = df['Hillshade_9am'].astype(int)

l=asarray(df['Hillshade_Noon'])
l1=np.reshape(l,(len(l),1))
enc = OrdinalEncoder()
enc.fit(l1)
result = enc.fit_transform(l1)
for x in range(len(result)):
  result[x]=int(result[x])
df['Hillshade_Noon']=result
df['Hillshade_Noon'] = df['Hillshade_Noon'].astype(int)

l=asarray(df['Horizontal_Distance_To_Fire_Points'])
l1=np.reshape(l,(len(l),1))
enc = OrdinalEncoder()
enc.fit(l1)
result = enc.fit_transform(l1)
for x in range(len(result)):
  result[x]=int(result[x])
df['Horizontal_Distance_To_Fire_Points']=result
df['Horizontal_Distance_To_Fire_Points'] = df['Horizontal_Distance_To_Fire_Points'].astype(int)

#displaying dataframe after performing ordinal encoding of all categorical columns
df.head()

"""**Feature Selection**"""

#separating input and target columns
properties = list(df.columns.values)
properties.remove('target')
X = df[properties]
y = df['target']

#Evaluating and plotting information gain of each input feature
imp=mutual_info_classif(X, y)
featImp=pd.Series(imp,df.columns[0:len(df.columns)-1])
featImp.plot(kind='bar',color='teal')
plt.title('Information Gain of the features')
plt.xlabel('Feature Name')
plt.ylabel('Information Gain Value')
plt.show()

#Dropping feature which have less than 0.1 information gain
featureNames=list(df.columns)
importantFeatures=list()
for i in range(len(imp)):
  if(imp[i]>0.1):
    importantFeatures.append(featureNames[i])

#Displaying features with information gain > 0.1
importantFeatures

#Creating dataframe which contains above features
df2=df[importantFeatures]

#Displaying dataframe that contains features with information gain >0.1
df2

"""**Performing TSNE(T-distributed Stochastic Neighbor Embedding)**"""

# normalize the values of all input features present in the df2 dataframe
X = whiten(df2)

#TSNE will reduce the number of features to two
from sklearn.manifold import TSNE
c_tsne = TSNE(n_components=2).fit_transform(X)
c_tsne=pd.DataFrame(c_tsne)

#Displaying dataframe created by TSNE that contains two features
print("0: Feature one created by TSNE")
print("1: Feature two created by TSNE")
c_tsne

"""**Implementing K-Means Algorithm**"""

#Splitting into training and testing data
X_train, X_test, y_train, y_test = train_test_split(c_tsne, y, test_size=0.2,random_state=9)

#Reseting index
X_train.reset_index(drop=True,inplace=True)
X_test.reset_index(drop=True,inplace=True)
y_train.reset_index(drop=True,inplace=True)
y_test.reset_index(drop=True,inplace=True)

#Displaying number of training and testing instances
print("Number of instances in Training dataset:",len(X_train.index))
print("Number of instances in Testing dataset: ",len(X_test.index))

#Creating K-Means Model and training on train dataset
kmeans = KMeans(n_clusters= 7,random_state=8)
clustersTrain = kmeans.fit_predict(X_train)
clustersTest  = kmeans.predict(X_test)
centroids = kmeans.cluster_centers_
centroids=pd.DataFrame(centroids)

#Displaying all the unique values of cluster numbers predicted
uniqueClusters=[]
for x in clustersTrain:
  if x not in uniqueClusters:
    uniqueClusters.append(x)
uniqueClusters

#Displaying cluster number with the number of instances predicted that value of cluster
print("Cluster Predicted",":","Number of instances")
for x in range(0,7):
  print("\t",x,"\t\t",list(clustersTrain).count(x))

#fetching true labels of all the instances of training dataset
trueLabelsTrain=y_train
uniqueTrueLabelsTrain =np.array(trueLabelsTrain)
trueLabelsTrain=trueLabelsTrain.to_list()

#Displaying unique true lables values
print("Unique values of True label:",np.unique(uniqueTrueLabelsTrain))

#function to find most frequent element in the list
def most_frequent(List):
    return max(set(List), key = List.count)

#Creating function "findMajorityLabels" that returns dictionary "majorityLabels" that will contain majority true label for each cluster
#In the dictionary "majorityLabels", key will be cluster and value will be its majority label
def findMajorityLabels(clustersTrain,trueLabelsTrain):
  majorityLabels={}
  for eachClusterValue in range(0,7):
    trueLabeslListForCurrentCluster=[]
    for eachClusterNumber in range(len(clustersTrain)):
      if clustersTrain[eachClusterNumber]==eachClusterValue:
        trueLabeslListForCurrentCluster.append(trueLabelsTrain[eachClusterNumber])
    majorityLabel=most_frequent(trueLabeslListForCurrentCluster)
    majorityLabels[eachClusterValue]=majorityLabel
  return majorityLabels

#creating dataframe for the storing cluster with its majority true label
majorityLabelDataframe=pd.DataFrame(columns=['Cluster Number','Majority Label'])
for clusterNumber,majorityLabel in majorityLabels.items():
  majorityLabelDataframe.loc[len(majorityLabelDataframe.index)] = [clusterNumber,majorityLabel]

#displaying majorityLabelDataframe
majorityLabelDataframe

#Finding corresponding true label for the training instances using predicted cluster 
predictedTrueLabelTrain=[]
for eachCluster in clustersTrain:
  predictedTrueLabelTrain.append(majorityLabels[eachCluster])

#Evaluating and displaying F1 score on the training instances
f1ScoreTrain=f1_score(predictedTrueLabelTrain, y_train,average='weighted')
print("F1 score on Training instances: ",f1ScoreTrain,"=","{0:.2f}".format(f1ScoreTrain*100),"%")

#Finding corresponding true label for the testing instances using predicted cluster 
predictedTrueLabelTest=[]
for eachCluster in clustersTest:
  predictedTrueLabelTest.append(majorityLabels[eachCluster])

#Evaluating and displaying F1 score on the testing instances
f1ScoreTest=f1_score(predictedTrueLabelTest, y_test,average='weighted')
print("F1 score on Testing instances: ",f1ScoreTest,"=","{0:.2f}".format(f1ScoreTest*100),"%")

#Evaluating and storing frequency of instances of each True label
trueLabelsCount=[0,0,0,0,0,0,0]
for eachTrueLabel in y:
  trueLabelsCount[eachTrueLabel-1]=trueLabelsCount[eachTrueLabel-1]+1
print("True Label","  ","Number of Instances")
for eachTrueLabelValue in range(1,8):
  print(eachTrueLabelValue,"\t\t",trueLabelsCount[eachTrueLabelValue-1])

#Evaluating the each true label count in each cluster
def ClusterTrueLabelCount(clusters,trueLabels):
  ClusterTrueLabelCounts=[]
  for eachClusterValue in range(0,7):
    trueLabeslListForCurrentCluster=[]
    for eachClusterNumber in range(len(clusters)):
      if clusters[eachClusterNumber]==eachClusterValue:
        trueLabeslListForCurrentCluster.append(trueLabels[eachClusterNumber])
    counts=[]
    for value in range(1,8):
      counts.append(trueLabeslListForCurrentCluster.count(value))
    ClusterTrueLabelCounts.append(counts)
  return ClusterTrueLabelCounts

#Comparison between true label counts in dataset and in individual cluster for each true label
clustersTrainList=clustersTrain.tolist()
clustersTestList=clustersTest.tolist()

clustersTrainList.extend(clustersTestList)
clustersCombined=clustersTrainList

trueLabelsTrain=y_train.to_list()
trueLabelsTest=y_test.to_list()
trueLabelsTrain.extend(trueLabelsTest)
trueLabelsCombined=trueLabelsTrain
l3=ClusterTrueLabelCount(clustersCombined,trueLabelsCombined)

df4=pd.DataFrame(l3)
df4=df4.T
df4 = df4.rename(columns={0:'Cluster 0 Count',1:'Cluster 1 Count',2:'Cluster 2 Count',3:'Cluster 3 Count',4:'Cluster 4 Count',5:'Cluster 5 Count',6:'Cluster 6 Count'})
df4['True Label']=[1,2,3,4,5,6,7]
df4.set_index('True Label',drop=True,inplace=True)
maxValuesOccured=df4.max(axis=1)
clusterNumberMaxValue=list(df4.idxmax(axis=1))
for q in range(len(clusterNumberMaxValue)):
  clusterNumberMaxValue[q]=clusterNumberMaxValue[q].replace('Count','')
df4['True Label Count']=trueLabelsCount
df4['Maximum Label Count Occured Together']=maxValuesOccured
df4['Cluster Number']=clusterNumberMaxValue
df4

#Displaying centroids
def displayCentroids(centroids):
  if(type(centroids) != list):
    centroids=centroids.to_numpy()
    centroids=centroids.tolist()
  centroidDataFrame=pd.DataFrame(centroids)
  centroidDataFrame['Cluster']=[0,1,2,3,4,5,6]
  centroidDataFrame = centroidDataFrame.rename(columns={0:'X Co-ordinate',1:'Y Co-ordinate'})
  centroidDataFrame.set_index('Cluster',drop=True,inplace=True)
  print(centroidDataFrame)

#Displaying cluster co-ordinates
displayCentroids(centroids)

#Visualization of clusters

#"XCombined" will store all training instances and all testing instances appended at the end of training instances
XCombined=X_train.append(X_test)

#"CombinedTrueClusters" will store all predicted clusters of training instances and all predicted clusters of testing append at end of training instances
trainClusters=clustersTrain.tolist()
testClusters=clustersTest.tolist()
trainClusters.extend(testClusters)
CombinedTrueClusters=trainClusters

#Adding predicted cluster column to the Xcombined datframe
XCombined['Cluster Predicted']=CombinedTrueClusters

#Filter rows of original data cluster wise
filtered_label0 = XCombined.loc[XCombined['Cluster Predicted'] == 0]
filtered_label1 = XCombined.loc[XCombined['Cluster Predicted'] == 1]
filtered_label2 = XCombined.loc[XCombined['Cluster Predicted'] == 2]
filtered_label3 = XCombined.loc[XCombined['Cluster Predicted'] == 3]
filtered_label4 = XCombined.loc[XCombined['Cluster Predicted'] == 4]
filtered_label5 = XCombined.loc[XCombined['Cluster Predicted'] == 5]
filtered_label6 = XCombined.loc[XCombined['Cluster Predicted'] == 6]

#Plotting the instances cluster wise
plt.scatter(filtered_label0.loc[:,0] , filtered_label0.loc[:,1] , color = 'yellow')
plt.scatter(filtered_label1.loc[:,0] , filtered_label1.loc[:,1] , color = 'red')
plt.scatter(filtered_label2.loc[:,0] , filtered_label2.loc[:,1] , color = 'cyan')
plt.scatter(filtered_label3.loc[:,0] , filtered_label3.loc[:,1] , color = 'magenta')
plt.scatter(filtered_label4.loc[:,0] , filtered_label4.loc[:,1] , color = 'bisque')
plt.scatter(filtered_label5.loc[:,0] , filtered_label5.loc[:,1] , color = 'orange')
plt.scatter(filtered_label6.loc[:,0] , filtered_label6.loc[:,1] , color = 'pink')
plt.scatter(centroids.loc[:,0] , centroids.loc[:,1] , color = 'black',s=100)
plt.xlabel("First Feature")
plt.ylabel("Second Feature")
plt.title("Clusters Visualization")
print("Note: Cluster Centroids are represented by Black Markers")
plt.show()

""" **Implementing BIRCH(Balanced Iterative Reducing and Clustering using Hierarchies) algorithm**"""

# Import required libraries and modules
import matplotlib.pyplot as plt
from sklearn.cluster import Birch

# Creating the BIRCH clustering model
model = Birch(branching_factor = 30, n_clusters = 7, threshold = 2)

# Fit the data (Training)
model.fit(X_train)

# Predict the same data

clustersTrain = model.predict(X_train)
clustersTest = model.predict(X_test)

#Evaluation Majoirty labels for each cluster predicted
maojorityLabels=findMajorityLabels(clustersTrain,y_train)

#creating dataframe for the storing cluster with its majority true label
majorityLabelDataframe=pd.DataFrame(columns=['Cluster Number','Majority Label'])
for clusterNumber,majorityLabel in majorityLabels.items():
  majorityLabelDataframe.loc[len(majorityLabelDataframe.index)] = [clusterNumber,majorityLabel]

#displaying majorityLabelDataframe
majorityLabelDataframe

#Finding corresponding true label for the training instances using predicted cluster 
predictedTrueLabelTrain=[]
for eachCluster in clustersTrain:
  predictedTrueLabelTrain.append(majorityLabels[eachCluster])

#Evaluating and displaying F1 score on the training instances
f1ScoreTrain=f1_score(predictedTrueLabelTrain, y_train,average='micro')
print("F1 score on Training instances: ",f1ScoreTrain,"=","{0:.2f}".format(f1ScoreTrain*100),"%")

#Finding corresponding true label for the testing instances using predicted cluster
predictedTrueLabelTest=[]
for eachCluster in clustersTest:
  predictedTrueLabelTest.append(majorityLabels[eachCluster])

#Evaluating and displaying F1 score on the testing instances
f1ScoreTest=f1_score(predictedTrueLabelTest, y_test,average='weighted')
print("F1 score on Testing instances: ",f1ScoreTest,"=","{0:.2f}".format(f1ScoreTest*100),"%")

#Comparison between true label counts in dataset and in individual cluster for each true label
clustersTrainList=clustersTrain.tolist()
clustersTestList=clustersTest.tolist()

clustersTrainList.extend(clustersTestList)
clustersCombined=clustersTrainList

trueLabelsTrain=y_train.to_list()
trueLabelsTest=y_test.to_list()
trueLabelsTrain.extend(trueLabelsTest)
trueLabelsCombined=trueLabelsTrain
l3=ClusterTrueLabelCount(clustersCombined,trueLabelsCombined)

df4=pd.DataFrame(l3)
df4=df4.T
df4 = df4.rename(columns={0:'Cluster 0 Count',1:'Cluster 1 Count',2:'Cluster 2 Count',3:'Cluster 3 Count',4:'Cluster 4 Count',5:'Cluster 5 Count',6:'Cluster 6 Count'})
df4['True Label']=[1,2,3,4,5,6,7]
df4.set_index('True Label',drop=True,inplace=True)
maxValuesOccured=df4.max(axis=1)
clusterNumberMaxValue=list(df4.idxmax(axis=1))
for q in range(len(clusterNumberMaxValue)):
  clusterNumberMaxValue[q]=clusterNumberMaxValue[q].replace('Count','')
df4['True Label Count']=trueLabelsCount
df4['Maximum Label Count Occured Together']=maxValuesOccured
df4['Cluster Number']=clusterNumberMaxValue
df4

#Visualization of clusters

#"XCombined" will store all training instances and all testing instances appended at the end of training instances
XCombined=X_train.append(X_test)

#"CombinedTrueClusters" will store all predicted clusters of training instances and all predicted clusters of testing append at end of training instances
trainClusters=clustersTrain.tolist()
testClusters=clustersTest.tolist()
trainClusters.extend(testClusters)
CombinedTrueClusters=trainClusters

#Adding predicted cluster column to the Xcombined datframe
XCombined['Cluster Predicted']=CombinedTrueClusters

#Filter rows of original data cluster wise
filtered_label0 = XCombined.loc[XCombined['Cluster Predicted'] == 0]
filtered_label1 = XCombined.loc[XCombined['Cluster Predicted'] == 1]
filtered_label2 = XCombined.loc[XCombined['Cluster Predicted'] == 2]
filtered_label3 = XCombined.loc[XCombined['Cluster Predicted'] == 3]
filtered_label4 = XCombined.loc[XCombined['Cluster Predicted'] == 4]
filtered_label5 = XCombined.loc[XCombined['Cluster Predicted'] == 5]
filtered_label6 = XCombined.loc[XCombined['Cluster Predicted'] == 6]

#Evaluating centroids of each cluster
centroidList=[]
l0=filtered_label0.mean(axis=0)
centroidList.append([l0[0],l0[1]])

l1=filtered_label1.mean(axis=0)
centroidList.append(list([l1[0],l1[1]]))

l2=filtered_label2.mean(axis=0)
centroidList.append(list([l2[0],l2[1]]))

l3=filtered_label3.mean(axis=0)
centroidList.append(list([l3[0],l3[1]]))

l4=filtered_label4.mean(axis=0)
centroidList.append(list([l4[0],l4[1]]))

l5=filtered_label5.mean(axis=0)
centroidList.append(list([l5[0],l5[1]]))

l6=filtered_label6.mean(axis=0)
centroidList.append(list([l6[0],l6[1]]))

#Plotting the instances cluster wise
plt.scatter(filtered_label0.loc[:,0] , filtered_label0.loc[:,1] , color = 'yellow')
plt.scatter(filtered_label1.loc[:,0] , filtered_label1.loc[:,1] , color = 'red')
plt.scatter(filtered_label2.loc[:,0] , filtered_label2.loc[:,1] , color = 'cyan')
plt.scatter(filtered_label3.loc[:,0] , filtered_label3.loc[:,1] , color = 'magenta')
plt.scatter(filtered_label4.loc[:,0] , filtered_label4.loc[:,1] , color = 'bisque')
plt.scatter(filtered_label5.loc[:,0] , filtered_label5.loc[:,1] , color = 'orange')
plt.scatter(filtered_label6.loc[:,0] , filtered_label6.loc[:,1] , color = 'pink')
#plt.scatter(centroids.loc[:,0] , centroids.loc[:,1] , color = 'black',s=100)
for eachCentroid in centroidList:
  plt.scatter(eachCentroid[0] , eachCentroid[1] , color = 'black',s=100)

plt.xlabel("First Feature")
plt.ylabel("Second Feature")
plt.title("Clusters Visualization")
print("Note: Cluster Centroids are represented by Black Markers")
plt.show()

#Displaying cluster co-ordinates
displayCentroids(centroidList)

"""**Implementing Fuzzy C Means Algorithm**"""

#Converting datasets to numpy array
X_train=X_train.to_numpy()
X_test=X_test.to_numpy()

#Creating and training model on training dataset
fcm = FCM(n_clusters=7,random_state=19)
fcm.fit(X_train)

#Storing centroids position and predicted clusters on training and testing instances
centroids = fcm.centers
clustersTrain = fcm.predict(X_train)
clustersTest= fcm.predict(X_test)

#Evaluating and storing majority label of each cluster
majorityLabels=findMajorityLabels(clustersTrain,y_train)

#creating dataframe for the storing cluster with its majority true label
majorityLabelDataframe=pd.DataFrame(columns=['Cluster Number','Majority Label'])
for clusterNumber,majorityLabel in majorityLabels.items():
  majorityLabelDataframe.loc[len(majorityLabelDataframe.index)] = [clusterNumber,majorityLabel]

#displaying majorityLabelDataframe
majorityLabelDataframe

#Finding corresponding true label for the training instances using predicted cluster 
predictedTrueLabelTrain=[]
for eachCluster in clustersTrain:
  predictedTrueLabelTrain.append(majorityLabels[eachCluster])

#Evaluating and displaying F1 score on the training instances
f1ScoreTrain=f1_score(predictedTrueLabelTrain, y_train,average='weighted')
print("F1 score on Training instances: ",f1ScoreTrain,"=","{0:.2f}".format(f1ScoreTrain*100),"%")

#Finding corresponding true label for the testing instances using predicted cluster
predictedTrueLabelTest=[]
for eachCluster in clustersTest:
  predictedTrueLabelTest.append(majorityLabels[eachCluster])

#Evaluating and displaying F1 score on the testing instances
f1ScoreTest=f1_score(predictedTrueLabelTest, y_test,average='weighted')
print("F1 score on Testing instances: ",f1ScoreTest,"=","{0:.2f}".format(f1ScoreTest*100),"%")

#Converting dataset to dataframe for visualization
X_train=pd.DataFrame(X_train)
X_test=pd.DataFrame(X_test)

#Comparison between true label counts in dataset and in individual cluster for each true label
clustersTrainList=clustersTrain.tolist()
clustersTestList=clustersTest.tolist()

clustersTrainList.extend(clustersTestList)
clustersCombined=clustersTrainList

trueLabelsTrain=y_train.to_list()
trueLabelsTest=y_test.to_list()
trueLabelsTrain.extend(trueLabelsTest)
trueLabelsCombined=trueLabelsTrain
l3=ClusterTrueLabelCount(clustersCombined,trueLabelsCombined)

df4=pd.DataFrame(l3)
df4=df4.T
df4 = df4.rename(columns={0:'Cluster 0 Count',1:'Cluster 1 Count',2:'Cluster 2 Count',3:'Cluster 3 Count',4:'Cluster 4 Count',5:'Cluster 5 Count',6:'Cluster 6 Count'})
df4['True Label']=[1,2,3,4,5,6,7]
df4.set_index('True Label',drop=True,inplace=True)
maxValuesOccured=df4.max(axis=1)
clusterNumberMaxValue=list(df4.idxmax(axis=1))
for q in range(len(clusterNumberMaxValue)):
  clusterNumberMaxValue[q]=clusterNumberMaxValue[q].replace('Count','')
df4['True Label Count']=trueLabelsCount
df4['Maximum Label Count Occured Together']=maxValuesOccured
df4['Cluster Number']=clusterNumberMaxValue
df4

#Displaying cluster co-ordinates
centroidDataFrame=pd.DataFrame(centroids)
centroidDataFrame['Cluster']=[0,1,2,3,4,5,6]
centroidDataFrame = centroidDataFrame.rename(columns={0:'X Co-ordinate',1:'Y Co-ordinate'})
centroidDataFrame.set_index('Cluster',drop=True,inplace=True)
print(centroidDataFrame)

#Visualization of clusters

#"XCombined" will store all training instances and all testing instances appended at the end of training instances
XCombined=X_train.append(X_test)

#"CombinedTrueClusters" will store all predicted clusters of training instances and all predicted clusters of testing append at end of training instances
trainClusters=clustersTrain.tolist()
testClusters=clustersTest.tolist()
trainClusters.extend(testClusters)
CombinedTrueClusters=trainClusters

#Adding predicted cluster column to the Xcombined datframe
XCombined['Cluster Predicted']=CombinedTrueClusters

#Filter rows of original data cluster wise
filtered_label0 = XCombined.loc[XCombined['Cluster Predicted'] == 0]
filtered_label1 = XCombined.loc[XCombined['Cluster Predicted'] == 1]
filtered_label2 = XCombined.loc[XCombined['Cluster Predicted'] == 2]
filtered_label3 = XCombined.loc[XCombined['Cluster Predicted'] == 3]
filtered_label4 = XCombined.loc[XCombined['Cluster Predicted'] == 4]
filtered_label5 = XCombined.loc[XCombined['Cluster Predicted'] == 5]
filtered_label6 = XCombined.loc[XCombined['Cluster Predicted'] == 6]


#Plotting the instances cluster wise
plt.scatter(filtered_label0.loc[:,0] , filtered_label0.loc[:,1] , color = 'yellow')
plt.scatter(filtered_label1.loc[:,0] , filtered_label1.loc[:,1] , color = 'springgreen')
plt.scatter(filtered_label2.loc[:,0] , filtered_label2.loc[:,1] , color = 'cyan')
plt.scatter(filtered_label3.loc[:,0] , filtered_label3.loc[:,1] , color = 'magenta')
plt.scatter(filtered_label4.loc[:,0] , filtered_label4.loc[:,1] , color = 'bisque')
plt.scatter(filtered_label5.loc[:,0] , filtered_label5.loc[:,1] , color = 'orange')
plt.scatter(filtered_label6.loc[:,0] , filtered_label6.loc[:,1] , color = 'pink')
#plt.scatter(centroids.loc[:,0] , centroids.loc[:,1] , color = 'black',s=100)

centroids=list(centroids)
for eachCentroid in centroids:
  plt.scatter(eachCentroid[0] , eachCentroid[1], color = 'black',s=100)

plt.xlabel("First Feature")
plt.ylabel("Second Feature")
plt.title("Clusters Visualization")
print("Note: Cluster Centroids are represented by Black Markers")
plt.show()

"""**Implementing Gaussian Mixture Clustering Algorithm**"""

#Creating and training model on training dataset
gmm = GaussianMixture(n_components = 7,random_state=5)
gmm.fit(X_train)

#Assign a label to each sample
clustersTrain = gmm.predict(X_train)
clustersTest  = gmm.predict(X_test)

#Evaluating and storing majority label of each cluster
majorityLabels=findMajorityLabels(clustersTrain,y_train)

#creating dataframe for the storing cluster with its majority true label
majorityLabelDataframe=pd.DataFrame(columns=['Cluster Number','Majority Label'])
for clusterNumber,majorityLabel in majorityLabels.items():
  majorityLabelDataframe.loc[len(majorityLabelDataframe.index)] = [clusterNumber,majorityLabel]

#displaying majorityLabelDataframe
majorityLabelDataframe

#Finding corresponding true label for the training instances using predicted cluster 
predictedTrueLabelTrain=[]
for eachCluster in clustersTrain:
  predictedTrueLabelTrain.append(majorityLabels[eachCluster])

#Evaluating and displaying F1 score on the training instances
f1ScoreTrain=f1_score(predictedTrueLabelTrain, y_train,average='weighted')
print("F1 score on Training instances: ",f1ScoreTrain,"=","{0:.2f}".format(f1ScoreTrain*100),"%")

#Finding corresponding true label for the testing instances using predicted cluster
predictedTrueLabelTest=[]
for eachCluster in clustersTest:
  predictedTrueLabelTest.append(majorityLabels[eachCluster])

#Evaluating and displaying F1 score on the testing instances
f1ScoreTest=f1_score(predictedTrueLabelTest, y_test,average='weighted')
print("F1 score on Testing instances: ",f1ScoreTest,"=","{0:.2f}".format(f1ScoreTest*100),"%")

#Converting dataset to dataframe for visualization
X_train=pd.DataFrame(X_train)
X_test=pd.DataFrame(X_test)

#Comparison between true label counts in dataset and in individual cluster for each true label
clustersTrainList=clustersTrain.tolist()
clustersTestList=clustersTest.tolist()

clustersTrainList.extend(clustersTestList)
clustersCombined=clustersTrainList

trueLabelsTrain=y_train.to_list()
trueLabelsTest=y_test.to_list()
trueLabelsTrain.extend(trueLabelsTest)
trueLabelsCombined=trueLabelsTrain
l3=ClusterTrueLabelCount(clustersCombined,trueLabelsCombined)

df4=pd.DataFrame(l3)
df4=df4.T
df4 = df4.rename(columns={0:'Cluster 0 Count',1:'Cluster 1 Count',2:'Cluster 2 Count',3:'Cluster 3 Count',4:'Cluster 4 Count',5:'Cluster 5 Count',6:'Cluster 6 Count'})
df4['True Label']=[1,2,3,4,5,6,7]
df4.set_index('True Label',drop=True,inplace=True)
maxValuesOccured=df4.max(axis=1)
clusterNumberMaxValue=list(df4.idxmax(axis=1))
for q in range(len(clusterNumberMaxValue)):
  clusterNumberMaxValue[q]=clusterNumberMaxValue[q].replace('Count','')
df4['True Label Count']=trueLabelsCount
df4['Maximum Label Count Occured Together']=maxValuesOccured
df4['Cluster Number']=clusterNumberMaxValue
df4

#Visualization of clusters

#"XCombined" will store all training instances and all testing instances appended at the end of training instances
XCombined=X_train.append(X_test)

#"CombinedTrueClusters" will store all predicted clusters of training instances and all predicted clusters of testing append at end of training instances
trainClusters=clustersTrain.tolist()
testClusters=clustersTest.tolist()
trainClusters.extend(testClusters)
CombinedTrueClusters=trainClusters

#Adding predicted cluster column to the Xcombined datframe
XCombined['Cluster Predicted']=CombinedTrueClusters

#Filter rows of original data cluster wise
filtered_label0 = XCombined.loc[XCombined['Cluster Predicted'] == 0]
filtered_label1 = XCombined.loc[XCombined['Cluster Predicted'] == 1]
filtered_label2 = XCombined.loc[XCombined['Cluster Predicted'] == 2]
filtered_label3 = XCombined.loc[XCombined['Cluster Predicted'] == 3]
filtered_label4 = XCombined.loc[XCombined['Cluster Predicted'] == 4]
filtered_label5 = XCombined.loc[XCombined['Cluster Predicted'] == 5]
filtered_label6 = XCombined.loc[XCombined['Cluster Predicted'] == 6]

#Evaluating centroids of each cluster
centroidList=[]
l0=filtered_label0.mean(axis=0)
centroidList.append([l0[0],l0[1]])

l1=filtered_label1.mean(axis=0)
centroidList.append(list([l1[0],l1[1]]))

l2=filtered_label2.mean(axis=0)
centroidList.append(list([l2[0],l2[1]]))

l3=filtered_label3.mean(axis=0)
centroidList.append(list([l3[0],l3[1]]))

l4=filtered_label4.mean(axis=0)
centroidList.append(list([l4[0],l4[1]]))

l5=filtered_label5.mean(axis=0)
centroidList.append(list([l5[0],l5[1]]))

l6=filtered_label6.mean(axis=0)
centroidList.append(list([l6[0],l6[1]]))

#Plotting the instances cluster wise
plt.scatter(filtered_label0.loc[:,0] , filtered_label0.loc[:,1] , color = 'yellow')
plt.scatter(filtered_label1.loc[:,0] , filtered_label1.loc[:,1] , color = 'springgreen')
plt.scatter(filtered_label2.loc[:,0] , filtered_label2.loc[:,1] , color = 'cyan')
plt.scatter(filtered_label3.loc[:,0] , filtered_label3.loc[:,1] , color = 'magenta')
plt.scatter(filtered_label4.loc[:,0] , filtered_label4.loc[:,1] , color = 'bisque')
plt.scatter(filtered_label5.loc[:,0] , filtered_label5.loc[:,1] , color = 'orange')
plt.scatter(filtered_label6.loc[:,0] , filtered_label6.loc[:,1] , color = 'pink')
#plt.scatter(centroids.loc[:,0] , centroids.loc[:,1] , color = 'black',s=100)

for eachCentroid in centroidList:
  plt.scatter(eachCentroid[0] , eachCentroid[1] , color = 'black',s=100)

plt.xlabel("First Feature")
plt.ylabel("Second Feature")
plt.title("Clusters Visualization")
print("Note: Cluster Centroids are represented by Black Markers")
plt.show()

#Displaying cluster co-ordinates
displayCentroids(centroidList)

"""**Saving Best Model**"""

#Saving the model to disk
import pickle
filename = 'bestModelDmgAssignment3Q2.sav'
pickle.dump(gmm, open(filename, 'wb'))
 
# # some time later...
 
# # load the model from disk
# loaded_model = pickle.load(open(filename, 'rb'))
# result = loaded_model.predict(X_train)
# print(len(result))

#Splitting into training and testing data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=9)
#Reseting index
X_train.reset_index(drop=True,inplace=True)
X_test.reset_index(drop=True,inplace=True)
y_train.reset_index(drop=True,inplace=True)
y_test.reset_index(drop=True,inplace=True)

y_test.to_csv('TrueLabelTestCSV.csv')

X_test.to_csv('testCSV.csv')